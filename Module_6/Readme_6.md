В проекте со мной участвовали:  
1) Zhulik2018(ник на github) https://www.kaggle.com/jane94. 
2) Alexander (ник slack) https://www.kaggle.com/susuber
# Проект 6. Выбираем авто выгодно. 
Задача проекта. 
Создать модель, которая будет предсказывать стоимость автомобиля по его характеристикам (признакам).

submission.csv - загруженный на Kaggle с метрикой MAPE 12%. 

APP6.ipynb – ноутбук для этого «сабмита». 

Choose_a_car_profitably2 (1) – ноутбук с кодом, показавшим худшую метрику, но со свежими данными, которые были получены парсингом. 

В результате удалось опробовать основные модели машинного обучения:  

  XGBoost и LightGBM очень мощные библиотеки для построения моделей градиентного бустинга на решающих деревьях.  
  Преимущества Light GBM:быстрая скорость обучения и высокая эффективность.  
  Алгоритм XGBoost обладает высокой предсказательной способностью и в разы быстрее любых других методов градиентного бустинга, включает 
в себя различные регуляризации, что уменьшает переобучение и улучшает общую производительность.  
  RandomForestRegressor имеет следующие преимущества:  
имеет высокую точность предсказания, которая сравнима с результатами градиентного бустинга;  
не требует тщательной настройки параметров, хорошо работает из коробки; редко переобучается.  
На практике добавление деревьев только улучшает композицию; хорошо работает с пропущенными данными – сохраняет хорошую точность даже при их наличии.  
  В Catboost прогнозы делаются на основе ансамбля слабых обучающих алгоритмов. В отличие от случайного леса, который создает дерево решений для каждой выборки,  
в градиентном бустинге деревья создаются последовательно. Предыдущие деревья в модели не изменяются.  
  Результаты предыдущего дерева используются для улучшения последующего.  
  В мире машинного обучения одними из самых популярных типов моделей являются решающее дерево и ансамбли на их основе.  
Преимуществами деревьев являются: простота интерпретации, нет ограничений на вид исходной зависимости, мягкие требования к размеру выборки.  
Деревья имеют и крупный недостаток — склонность к переобучению. Поэтому почти всегда деревья объединяют в ансамбли: случайный лес, градиентный бустинг и др.  
Сложными теоретическими и практическим задачами являются составление деревьев и объединение их в ансамбли.  
  Extra Trees — это метод ансамблевого обучения, который объединяет результаты нескольких не коррелированных деревьев решений, собранных в «лесу», для вывода результатов.  
Лучшим из рассмотренных себя проявил staking моделей Catboost, XGBoost,Extra Trees и LightGBM с мета моделью линейной регрессии. В процессе работы оказалось справедливом следующее предположение:  
поскольку в staking мета-модель обучается на ответах уже натренированных алгоритмов, то они сильно коррелируют.  
Для борьбы с этим часто базовые алгоритмы не сильно оптимизируют как и было сделано в нашем случае

